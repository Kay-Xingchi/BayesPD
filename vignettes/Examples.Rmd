---
title: "Examples"
author: "Xingchi Li"
date: "5/7/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
  rmarkdown::html_vignette: default
vignette: |
  %\VignetteIndexEntry{Examples} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(BayesPD)
```

## beta_sample_comparison TODO

In Bayesian Analysis, we sometimes need to compare the output of the beta distributions, so this function simplifies the process. All you need to do is supplied the sample size, and parameters for the beta distribution, and it will return the probability that the sample of one distribution is smaller than the other.

There are two groups of parameters required, $beta(a_1,b_1)$ and $beta(a_2,b_2)$. Only one of them can be vectors of length bigger than 1.

```{r, results="hide"}
beta_sample_comparison(sample_size = 9000, beta_a1 = 50:60, beta_b1 = 40:50, beta_a2 = 31, beta_b2 = 21, plot = TRUE)
```

The example shows that we have ten pairs of $beta(a_1,b_1)$, and we want to know how the output of $beta(a_1,b_1)$ compare to the output of $beta(a_2,b_2)$ with $beta(a_1,b_1)$ changing.

## gamma_poisson_ppd

Given the data: number of children of men in their $30$s with and without bachelor's degrees, respectively. 

Let $\theta_A$ and $\theta_B$ be the average of each group. Using a Poisson sampling model, a $gamma(2, 1)$ prior for each $\theta$ and the data, obtain $5000$ samples of $\tilde{Y}_A$ and $\tilde{Y}_B$ from the posterior predictive distribution of the two samples. We have the Monte Carlo approximations to these two posterior predictive distributions in the first two pictures.

```{r, results="hide", cache=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60), fig.height=4, fig.width=3}
out <- gamma_poisson_ppd(sample_size = 5000, gamma_a1 = 2, gamma_b1 = 1, y1 = menchild30bach, gamma_a2 = 2, gamma_b2 = 1, y2 = menchild30nobach, confidence_interval = c(0.025, 0.975), using_MCMC = TRUE, poisson_fitting_mean = 1.4)
```

The black lines in the third and fourth figures are the empirical distribution and the red lines are the distribution of Poisson with mean 1.4. As we can see, the Poisson model with mean 1.4 is not a proper model.

For each of the 5000 $\theta_B$ values we sampled, sample $218$ Poisson random variables and count the number of $0$s and the number of $1$s in each of the $5000$ simulated datasets. Now we have two sequences of length $5000$ each, one sequence counting the number of people having zero children for each of the $5000$ posterior predictive datasets, the other counting the number of people with one child. The fifth figure plots the two sequences against one another (one on the x-axis, one on the y-axis). The red point marks how many people in the observed dataset had zero children and one child.

The 95% credible interval for $\theta_B-\theta_A$ is

```{r, dependson=-1}
out$theta2_minus_theta1_quantile
```

And the 95% credible interval for $\tilde{Y}_B-\tilde{Y}_A$ is

```{r, dependson=-2}
out$y_tidle2_minus_ytilde1_quantile
```

## posterior_predictive_sample_comparison

Given two Poisson models, the parameters have $gamma(237,20)$ prior distribution, and variable gamma prior distributions. Using MCMC sampling $5000$ samples, we can see the going of the probability that $\tilde{Y}_A>\tilde{Y}_B$ with the parameters for the second prior changing.

```{r, results="hide", cache=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60), fig.height=4}
out <- posterior_predictive_sample_comparison(sample_size = 5000, gamma_a1 = 237, gamma_b1 = 20, gamma_a2 = 12*(1:100)+113, gamma_b2 = 13+(1:100), tildeya_smaller_than_tildeyb = FALSE, plot = TRUE)
```

The output of the function is just the probability.

## normal_gamma_conjugate_family

Given data: school1, school2, school3 containing the amount of time students from three high schools spent on studying or homework during an exam period. Using the normal model with a conjugate prior distribution, in which $\{\mu_0 = 5, \sigma_0^2 = 4, \kappa_0 = 1, \nu_0 = 2\}$.

```{r, results="hide", cache=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
out <- normal_gamma_conjugate_family(sample_size = 10000, mu_0 = 5, sigma_0_sequare = 4, kappa_0 = 1, nu_0 = 2, y.1 = school1, y.2 = school2, y.3 = school3, confidence_interval = c(0.025, 0.975))
```

The posterior mean of $\theta$ are: 

```{r, dependson=-1}
out$inference.1$theta_bar.1
out$inference.2$theta_bar.2
out$inference.3$theta_bar.3
```

The posterior mean of $\sigma$ are:

```{r, dependson=-2}
out$inference.1$sigma_bar.1
out$inference.2$sigma_bar.2
out$inference.3$sigma_bar.3
```

The 95% confidence interval for the mean $\theta$ are:

```{r, dependson=-3}
out$inference.1$confidence_interval_theta.1
out$inference.2$confidence_interval_theta.2
out$inference.3$confidence_interval_theta.3
```

The 95% confidence interval for the standard deviation $\sigma$ are:

```{r, dependson=-4}
out$inference.1$confidence_interval_sigma_bar.1
out$inference.2$confidence_interval_sigma_bar.2
out$inference.3$confidence_interval_sigma_bar.3
```

The posterior probability that $\theta_1<\theta_2<\theta_3$ (here 1, 2, 3 are permutations and can be exchanged freely) is:

```{r, dependson=-5}
out$theta_smaller.1.2.3
```

The posterior probability that $\tilde{Y}_1<\tilde{Y}_3<\tilde{Y}_2$ (all six permutations as well) is:

```{r, dependson=-6}
out$y_tilde_smaller.1.3.2
```

The posterior probability that $\theta_1$ is bigger than both $\theta_2$ and $\theta_3$ is:

```{r, dependson=-7}
out$theta_biggest.1
```

The posterior probability that $\tilde{Y}_1$ is bigger than both $\tilde{Y}_2$ and $\tilde{Y}_3$ is:

```{r, dependson=-8}
out$y_tilde_biggest.1
```
```
